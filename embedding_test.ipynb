{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:32:55.485516Z",
     "start_time": "2024-10-27T10:32:46.944797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import path, rename, mkdir, listdir, makedirs\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "datasets.utils.tqdm = tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=\"dAiv-ai-competition-2024-pro\")\n",
    "\n",
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 7\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    random_state = 20241028\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train or valid:  # Split Train and Validation Set\n",
    "            seperated = train_test_split(\n",
    "                self.samples, self.targets, test_size=1-split_ratio, stratify=self.targets, random_state=self.random_state\n",
    "            )\n",
    "            self.samples, self.targets = (seperated[0], seperated[2]) if train else (seperated[1], seperated[3])\n",
    "            self.imgs = self.samples\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "\n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")"
   ],
   "id": "d6a793f650e9224a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrnoro5122\u001B[0m (\u001B[33mrnoro5122-chungnam-national-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/shared_hdd/rnoro5122/dAiv_AI_Competition_Pro_2024/wandb/run-20241027_103253-ckjqswbw</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/ckjqswbw' target=\"_blank\">earthy-thunder-22</a></strong> to <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro' target=\"_blank\">https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/ckjqswbw' target=\"_blank\">https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/ckjqswbw</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-27T10:32:56.670986Z",
     "start_time": "2024-10-27T10:32:56.533967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries and load Datasets has already been completed.\n",
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])\n",
    "\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")\n",
    "\n",
    "#Data Augmentation if needed\n",
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1\n",
    "\n",
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")\n",
    "\n",
    "#Label Transform\n",
    "CLASS_LABELS = len(train_dataset.classes) + 1\n",
    "COMBINATION_AXIS = 2\n",
    "\n",
    "import itertools\n",
    "\n",
    "class LabelTransformer:\n",
    "    def __init__(self, num_classes: int, comb_axis: int):\n",
    "        self.num_classes = num_classes\n",
    "        self.comb_axis = comb_axis\n",
    "        self.combinations = [(-1, n) for n in (*range(num_classes), -2)] + list(itertools.combinations((-2, *range(num_classes)), comb_axis))\n",
    "        self.num_combinations = len(self.combinations)\n",
    "\n",
    "    def find(self, comb_id):\n",
    "        return self.combinations[comb_id]\n",
    "\n",
    "label_transformer = LabelTransformer(CLASS_LABELS, COMBINATION_AXIS)\n",
    "\n",
    "#DataLoader\n",
    "BATCH_SIZE = 300 # Set Batch Size\n",
    "\n",
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "\n",
    "#Define Model\n",
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.scale = 1 / math.sqrt(hidden_dim)\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         # x의 형태: (batch_size, seq_len, hidden_dim)\n",
    "#         Q = self.query(x)  # (batch_size, seq_len, hidden_dim)\n",
    "#         K = self.key(x)    # (batch_size, seq_len, hidden_dim)\n",
    "#         V = self.value(x)  # (batch_size, seq_len, hidden_dim)\n",
    "# \n",
    "#         # Scaled Dot-Product Attention\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (batch_size, seq_len, seq_len)\n",
    "#         attention_weights = self.softmax(scores)  # (batch_size, seq_len, seq_len)\n",
    "#         context = torch.matmul(attention_weights, V)  # (batch_size, seq_len, hidden_dim)\n",
    "#         return context\n",
    "\n",
    "# class VisualEmbedding(nn.Module):\n",
    "#     \"\"\" Visual Embedding Model \"\"\"\n",
    "# \n",
    "#     def __init__(self, embedding_dim: int):\n",
    "#         super().__init__()\n",
    "# \n",
    "#         self.resnet = models.resnet34(pretrained=True)\n",
    "#         self.resnet.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "#         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.resnet(x)\n",
    "\n",
    "# class VisualEmbedding(nn.Module):\n",
    "#     \"\"\" Visual Embedding Model \"\"\"\n",
    "#     def __init__(self, embedding_dim: int):\n",
    "#         super().__init__()\n",
    "#         resnet = models.resnet34(pretrained=True)\n",
    "#         self.features = nn.Sequential(*list(resnet.children())[:-2])  # 마지막 두 레이어 제거\n",
    "#         self.conv = nn.Conv2d(512, embedding_dim, kernel_size=1)  # 채널 수 조정\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)  # (batch_size, 512, H, W)\n",
    "#         x = self.conv(x)      # (batch_size, embedding_dim, H, W)\n",
    "#         return x\n",
    "\n",
    "# class ImageClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim: int, comb_axis: int, num_combinations: int, num_classes: int):\n",
    "#         super().__init__()\n",
    "# \n",
    "#         # Visual Embedding\n",
    "#         self.visual_embedding = VisualEmbedding(embedding_dim)\n",
    "#         self.hidden_size = embedding_dim\n",
    "#         self.semantic_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(self.hidden_size),\n",
    "#             nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(self.hidden_size // 2, comb_axis),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#         self.converter = nn.Linear(comb_axis, num_combinations)\n",
    "# \n",
    "#     def forward(self, x) -> torch.Tensor:\n",
    "#         embedding = self.visual_embedding(x)\n",
    "#         embedding = self.semantic_embedding(embedding)\n",
    "#         logits = self.converter(embedding)\n",
    "#         return logits\n",
    "\n",
    "# class ImageClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim: int, comb_axis: int, num_combinations: int, num_classes: int):\n",
    "#         super().__init__()\n",
    "#         self.visual_embedding = VisualEmbedding(embedding_dim)\n",
    "#         self.self_attention = SelfAttention(embedding_dim)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.hidden_size = embedding_dim\n",
    "#         self.semantic_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(self.hidden_size),\n",
    "#             nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(self.hidden_size // 2, comb_axis),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#         self.converter = nn.Linear(comb_axis, num_combinations)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         feature_map = self.visual_embedding(x)  # (batch_size, embedding_dim, H, W)\n",
    "#         batch_size, embedding_dim, H, W = feature_map.size()\n",
    "# \n",
    "#         # 피처 맵을 시퀀스로 변환\n",
    "#         feature_seq = feature_map.view(batch_size, embedding_dim, -1).permute(0, 2, 1)  # (batch_size, seq_len, embedding_dim)\n",
    "# \n",
    "#         # Self-Attention 적용\n",
    "#         attended_features = self.self_attention(feature_seq)  # (batch_size, seq_len, embedding_dim)\n",
    "# \n",
    "#         # 시퀀스를 하나의 벡터로 변환 (평균 풀링)\n",
    "#         embedding = torch.mean(attended_features, dim=1)  # (batch_size, embedding_dim)\n",
    "# \n",
    "#         # 기존의 semantic_embedding과 converter 적용\n",
    "#         embedding = self.semantic_embedding(embedding)\n",
    "#         logits = self.converter(embedding)\n",
    "#         return logits"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset loaded successfully. Number of samples - Train(7478), Valid(1870), Test(1110), Unlabeled(380)\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Train dataset has been overridden with augmented state. Number of samples - Train(7478)\n",
      "INFO: Number of CPU cores - 48\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:32:56.724569Z",
     "start_time": "2024-10-27T10:32:56.704550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import models\n",
    "\n",
    "# Multi-Head Self-Attention 클래스 정의\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim은 num_heads로 나누어 떨어져야 합니다.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # 쿼리, 키, 밸류 계산\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Multi-Head로 형태 변환\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # 헤드 연결\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        output = self.out_proj(context)  # (batch_size, seq_len, hidden_dim)\n",
    "        return output\n",
    "\n",
    "# 2D 위치 인코딩 클래스 정의\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        # 위치 좌표 생성\n",
    "        y_pos = torch.linspace(0, 1, steps=height, device=device).unsqueeze(1).repeat(1, width)\n",
    "        x_pos = torch.linspace(0, 1, steps=width, device=device).unsqueeze(0).repeat(height, 1)\n",
    "        y_pos = y_pos.unsqueeze(0).unsqueeze(0).repeat(batch_size, self.hidden_dim // 2, 1, 1)\n",
    "        x_pos = x_pos.unsqueeze(0).unsqueeze(0).repeat(batch_size, self.hidden_dim // 2, 1, 1)\n",
    "\n",
    "        # 위치 인코딩 생성\n",
    "        pos_encoding = torch.cat([x_pos, y_pos], dim=1)\n",
    "        return pos_encoding  # (batch_size, hidden_dim, height, width)\n",
    "\n",
    "# Visual Embedding 클래스 정의\n",
    "class VisualEmbedding(nn.Module):\n",
    "    \"\"\"Visual Embedding Model\"\"\"\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # 마지막 두 레이어 제거\n",
    "        self.conv = nn.Conv2d(512, embedding_dim, kernel_size=1)  # 채널 수 조정\n",
    "        self.bn = nn.BatchNorm2d(embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # (batch_size, 512, H, W)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x  # (batch_size, embedding_dim, H, W)\n",
    "\n",
    "# ImageClassifier 클래스 정의\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, comb_axis: int, num_combinations: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.visual_embedding = VisualEmbedding(embedding_dim)\n",
    "        self.position_encoding = PositionalEncoding2D(embedding_dim)\n",
    "        self.self_attention = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            MultiHeadSelfAttention(embedding_dim, num_heads=8),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.hidden_size = embedding_dim\n",
    "        self.semantic_embedding = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.hidden_size // 2, comb_axis),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.converter = nn.Linear(comb_axis, num_combinations)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.visual_embedding(x)  # (batch_size, embedding_dim, H, W)\n",
    "        pos_encoding = self.position_encoding(feature_map)\n",
    "        feature_map = feature_map + pos_encoding  # 위치 인코딩 추가\n",
    "\n",
    "        batch_size, embedding_dim, H, W = feature_map.size()\n",
    "        feature_seq = feature_map.view(batch_size, embedding_dim, -1).permute(0, 2, 1)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Self-Attention 적용\n",
    "        attended_features = self.self_attention(feature_seq)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Residual Connection\n",
    "        attended_features = attended_features + feature_seq\n",
    "\n",
    "        # Global Max Pooling\n",
    "        embedding, _ = torch.max(attended_features, dim=1)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Semantic Embedding과 Converter 적용\n",
    "        embedding = self.semantic_embedding(embedding)\n",
    "        logits = self.converter(embedding)\n",
    "        return logits  # (batch_size, num_combinations)"
   ],
   "id": "7802e6661d6fd694",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T10:32:56.783276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBEDDING_DIM = 24  # 8~16: log(labels)\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    embedding_dim=EMBEDDING_DIM, comb_axis=COMBINATION_AXIS,\n",
    "    num_combinations=label_transformer.num_combinations, num_classes=CLASS_LABELS\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model_id = \"visual_embedding_attention\"\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 400\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCH)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "# 모델 저장 및 불러오는 함수 정의\n",
    "def save_checkpoint(epoch, model, optimizer, loss, PATH):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, PATH)\n",
    "    print(f\" Model saved.\")\n",
    "\n",
    "def load_checkpoint(PATH, model, optimizer):\n",
    "    if path.isfile(PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"체크포인트 '{path.basename(PATH)}'에서 모델 로드 완료 (시작 에포크: {start_epoch})\")\n",
    "        return start_epoch, loss\n",
    "    else:\n",
    "        print(f\"체크포인트 '{path.basename(PATH)}'를 찾을 수 없습니다. 새로 훈련을 시작합니다.\")\n",
    "        return 0, None\n",
    "\n",
    "#Traning Loop\n",
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "PATH = path.join('checkpoints', f\"{model_id}_checkpoint.pt.tar\") # 모델 체크포인트 저장 경로\n",
    "# 해당 경로에 폴더가 없을 경우 폴더 생성\n",
    "makedirs('checkpoints', exist_ok=True)\n",
    "save_cycle = 5\n",
    "\n",
    "# 체크포인트 로드\n",
    "start_epoch, _ = load_checkpoint(PATH, model, optimizer)\n",
    "\n",
    "epochs = tqdm(range(start_epoch, EPOCH), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "      tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()  # Update Learning Rate\n",
    "\n",
    "            train_progress.update(1)\n",
    "            #if i != train_length-1: wandb.log({'Loss': loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCH}], Step [{i+1:2}/{train_length}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / len(valid_dataset)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        #wandb.log({'Loss': loss.item(), 'Val Acc': val_acc, 'Val Loss': val_loss})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCH}], Step [{train_length}/{train_length}], Loss: {loss.item():.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCH else \"\")\n",
    "\n",
    "        # save_cycle마다 모델 저장\n",
    "        if (epoch + 1) % save_cycle == 0:\n",
    "            save_checkpoint(epoch, model, optimizer, loss.item(), PATH)"
   ],
   "id": "eb7c7470f9ddd49a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "체크포인트 'visual_embedding_attention_checkpoint.pt.tar'를 찾을 수 없습니다. 새로 훈련을 시작합니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Running Epochs:   0%|          | 0/400 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa5f1fe4cbb54c09ac4348db68963679"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/25 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10b5163d7bc7485bb1d0a8d747abadf7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1834145447a84294a0a9f41dcd54795f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 4/400], Step [20/25], Loss: 9.178624, Valid Acc: 0.000000%, Valid Loss: 9.114741"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"visual_embedding.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "be3e0570ffceaef3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
