{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f4de42",
   "metadata": {},
   "source": "# dAiv AI_Competition[2024]_Pro"
  },
  {
   "cell_type": "markdown",
   "id": "f9142930",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:02.371244Z",
     "start_time": "2024-10-27T08:00:02.367395Z"
    }
   },
   "cell_type": "code",
   "source": "# %pip install pygwalker wandb",
   "id": "bd374cd8ed6f6ddc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bd45d5b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:11.555065Z",
     "start_time": "2024-10-27T08:00:02.415124Z"
    }
   },
   "source": [
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "datasets.utils.tqdm = tqdm\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:14.575610Z",
     "start_time": "2024-10-27T08:00:12.539246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# WandB Initialization\n",
    "wandb.init(project=\"dAiv-ai-competition-2024-pro\")"
   ],
   "id": "a3934456c3880d88",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrnoro5122\u001B[0m (\u001B[33mrnoro5122-chungnam-national-university\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/shared_hdd/rnoro5122/dAiv_AI_Competition_Pro_2024/wandb/run-20241027_080013-tqmuaz3r</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/tqmuaz3r' target=\"_blank\">northern-river-5</a></strong> to <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro' target=\"_blank\">https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/tqmuaz3r' target=\"_blank\">https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/tqmuaz3r</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rnoro5122-chungnam-national-university/dAiv-ai-competition-2024-pro/runs/tqmuaz3r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f585a3f8440>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4049b434",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "e475e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:15.507834Z",
     "start_time": "2024-10-27T08:00:14.613958Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 08:00:14 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   47C    P0   201W / 250W |  15918MiB / 16280MiB |     19%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    31W / 250W |  15369MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    34W / 250W |    661MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    32W / 250W |  15293MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    32W / 250W |   7941MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0    33W / 250W |  15957MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   53C    P0   148W / 250W |  11631MiB / 16280MiB |     43%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    35W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     28310      C   ...on04/anaconda3/bin/python    15239MiB |\r\n",
      "|    1   N/A  N/A      9377      C   ...on08/anaconda3/bin/python    15367MiB |\r\n",
      "|    2   N/A  N/A     13010      C   ...on03/anaconda3/bin/python      659MiB |\r\n",
      "|    3   N/A  N/A     40435      C   ...on01/anaconda3/bin/python    15291MiB |\r\n",
      "|    4   N/A  N/A     24324      C   ...on08/anaconda3/bin/python     7939MiB |\r\n",
      "|    5   N/A  N/A     19929      C   ...on06/anaconda3/bin/python    15955MiB |\r\n",
      "|    6   N/A  N/A     25026      C   ...on02/anaconda3/bin/python    11629MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0d2df737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:15.886570Z",
     "start_time": "2024-10-27T08:00:15.548257Z"
    }
   },
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 7\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "85bc3d0f",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "id": "56b28aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:16.408438Z",
     "start_time": "2024-10-27T08:00:15.921059Z"
    }
   },
   "source": [
    "from typing import Callable, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    random_state = 20241028\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train or valid:  # Split Train and Validation Set\n",
    "            seperated = train_test_split(\n",
    "                self.samples, self.targets, test_size=1-split_ratio, stratify=self.targets, random_state=self.random_state\n",
    "            )\n",
    "            self.samples, self.targets = (seperated[0], seperated[2]) if train else (seperated[1], seperated[3])\n",
    "            self.imgs = self.samples\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "\n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Initialization",
   "id": "b419ce2cb5914ac5"
  },
  {
   "cell_type": "code",
   "id": "e1489d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:16.470612Z",
     "start_time": "2024-10-27T08:00:16.465342Z"
    }
   },
   "source": [
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "55933cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:16.671355Z",
     "start_time": "2024-10-27T08:00:16.552497Z"
    }
   },
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset loaded successfully. Number of samples - Train(7478), Valid(1870), Test(1110), Unlabeled(380)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualize Dataset Distribution\n",
    "    - for checking..."
   ],
   "id": "5b6fe305cb5b234b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:16.720538Z",
     "start_time": "2024-10-27T08:00:16.708415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Label Check\n",
    "for i, label in zip(range(5), train_dataset.targets):\n",
    "    print(i, \"-\", train_dataset.classes[label])"
   ],
   "id": "2a012af140f88d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 050.Pelagic_Cormorant\n",
      "1 - 020.Leonberger\n",
      "2 - 104.Black_capped_Vireo\n",
      "3 - 020.Leonberger\n",
      "4 - 009.Siamese\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:16.818528Z",
     "start_time": "2024-10-27T08:00:16.793574Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.df",
   "id": "ba2d5dcc2de29df3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             path                   label\n",
       "0      ./data/train/050.Pelagic_Cormorant/016.jpg   050.Pelagic_Cormorant\n",
       "1             ./data/train/020.Leonberger/162.jpg          020.Leonberger\n",
       "2     ./data/train/104.Black_capped_Vireo/030.jpg  104.Black_capped_Vireo\n",
       "3             ./data/train/020.Leonberger/059.jpg          020.Leonberger\n",
       "4                ./data/train/009.Siamese/120.jpg             009.Siamese\n",
       "...                                           ...                     ...\n",
       "7473          ./data/train/023.Pomeranian/061.jpg          023.Pomeranian\n",
       "7474            ./data/train/019.Keeshond/013.jpg            019.Keeshond\n",
       "7475               ./data/train/012.Boxer/156.jpg               012.Boxer\n",
       "7476    ./data/train/039.Brewer_Blackbird/009.jpg    039.Brewer_Blackbird\n",
       "7477         ./data/train/066.Eared_Grebe/021.jpg         066.Eared_Grebe\n",
       "\n",
       "[7478 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/train/050.Pelagic_Cormorant/016.jpg</td>\n",
       "      <td>050.Pelagic_Cormorant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/train/020.Leonberger/162.jpg</td>\n",
       "      <td>020.Leonberger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/train/104.Black_capped_Vireo/030.jpg</td>\n",
       "      <td>104.Black_capped_Vireo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/train/020.Leonberger/059.jpg</td>\n",
       "      <td>020.Leonberger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/train/009.Siamese/120.jpg</td>\n",
       "      <td>009.Siamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>./data/train/023.Pomeranian/061.jpg</td>\n",
       "      <td>023.Pomeranian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>./data/train/019.Keeshond/013.jpg</td>\n",
       "      <td>019.Keeshond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7475</th>\n",
       "      <td>./data/train/012.Boxer/156.jpg</td>\n",
       "      <td>012.Boxer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>./data/train/039.Brewer_Blackbird/009.jpg</td>\n",
       "      <td>039.Brewer_Blackbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>./data/train/066.Eared_Grebe/021.jpg</td>\n",
       "      <td>066.Eared_Grebe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7478 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:17.126379Z",
     "start_time": "2024-10-27T08:00:16.909471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "id": "d8c7a2e166c6556a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-00062570be2559f5HGQPp7Mjz936omLY\" style=\"height: auto\">\\n    <hea…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f65b15e5ea64e6299136def4b1ed546"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"00062570be2559f5HGQPp7Mjz936omLY\") {\n",
       "            var iframe = document.getElementById(\"gwalker-00062570be2559f5HGQPp7Mjz936omLY\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pygwalker.api.pygwalker.PygWalker at 0x7f584b5051f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:17.459518Z",
     "start_time": "2024-10-27T08:00:17.447924Z"
    }
   },
   "cell_type": "code",
   "source": "valid_dataset.df",
   "id": "c16ee12b11a6719",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            path                  label\n",
       "0      ./data/train/071.Evening_Grosbeak/034.jpg   071.Evening_Grosbeak\n",
       "1           ./data/train/066.Eared_Grebe/004.jpg        066.Eared_Grebe\n",
       "2             ./data/train/118.Rock_Wren/033.jpg          118.Rock_Wren\n",
       "3     ./data/train/035.Groove_billed_Ani/022.jpg  035.Groove_billed_Ani\n",
       "4          ./data/train/008.Russian_Blue/098.jpg       008.Russian_Blue\n",
       "...                                          ...                    ...\n",
       "1865              ./data/train/012.Boxer/000.jpg              012.Boxer\n",
       "1866     ./data/train/016.Great_Pyrenees/003.jpg     016.Great_Pyrenees\n",
       "1867                ./data/train/024.Pug/027.jpg                024.Pug\n",
       "1868       ./data/train/005.Egyptian_Mau/134.jpg       005.Egyptian_Mau\n",
       "1869          ./data/train/028.Shiba_Inu/110.jpg          028.Shiba_Inu\n",
       "\n",
       "[1870 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/train/071.Evening_Grosbeak/034.jpg</td>\n",
       "      <td>071.Evening_Grosbeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/train/066.Eared_Grebe/004.jpg</td>\n",
       "      <td>066.Eared_Grebe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/train/118.Rock_Wren/033.jpg</td>\n",
       "      <td>118.Rock_Wren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/train/035.Groove_billed_Ani/022.jpg</td>\n",
       "      <td>035.Groove_billed_Ani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/train/008.Russian_Blue/098.jpg</td>\n",
       "      <td>008.Russian_Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>./data/train/012.Boxer/000.jpg</td>\n",
       "      <td>012.Boxer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>./data/train/016.Great_Pyrenees/003.jpg</td>\n",
       "      <td>016.Great_Pyrenees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>./data/train/024.Pug/027.jpg</td>\n",
       "      <td>024.Pug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>./data/train/005.Egyptian_Mau/134.jpg</td>\n",
       "      <td>005.Egyptian_Mau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>./data/train/028.Shiba_Inu/110.jpg</td>\n",
       "      <td>028.Shiba_Inu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1870 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:17.767507Z",
     "start_time": "2024-10-27T08:00:17.609289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Valid Dataset Distribution\n",
    "walker = pyg.walk(valid_dataset.df, theme_key=\"streamlit\")"
   ],
   "id": "ab82cd5bc721d19c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-00062570be300ff88v5jFsAkVLDEJYBz\" style=\"height: auto\">\\n    <hea…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4ec6e8be762426cb793f45a28ecb007"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"00062570be300ff88v5jFsAkVLDEJYBz\") {\n",
       "            var iframe = document.getElementById(\"gwalker-00062570be300ff88v5jFsAkVLDEJYBz\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7b34674f",
   "metadata": {},
   "source": [
    "## Data Augmentation if needed"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ced1da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:17.965118Z",
     "start_time": "2024-10-27T08:00:17.960445Z"
    }
   },
   "source": [
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "268d5c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.079651Z",
     "start_time": "2024-10-27T08:00:18.019232Z"
    }
   },
   "source": [
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    resizer\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "efd6c21a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.148580Z",
     "start_time": "2024-10-27T08:00:18.107641Z"
    }
   },
   "source": [
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Train dataset has been overridden with augmented state. Number of samples - Train(7478)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label Transform",
   "id": "737aecca5ac9dea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.176349Z",
     "start_time": "2024-10-27T08:00:18.172774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLASS_LABELS = len(train_dataset.classes) + 1\n",
    "COMBINATION_AXIS = 2"
   ],
   "id": "2b9326f9c592cdfd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.234264Z",
     "start_time": "2024-10-27T08:00:18.226575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "class LabelTransformer:\n",
    "    def __init__(self, num_classes: int, comb_axis: int):\n",
    "        self.num_classes = num_classes\n",
    "        self.comb_axis = comb_axis\n",
    "        self.combinations = [(-1, n) for n in (*range(num_classes), -2)] + list(itertools.combinations((-2, *range(num_classes)), comb_axis))\n",
    "        self.num_combinations = len(self.combinations)\n",
    "        \n",
    "    def find(self, comb_id):\n",
    "        return self.combinations[comb_id]"
   ],
   "id": "ca5cfc4b1e2338e8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.295211Z",
     "start_time": "2024-10-27T08:00:18.288788Z"
    }
   },
   "cell_type": "code",
   "source": "label_transformer = LabelTransformer(CLASS_LABELS, COMBINATION_AXIS)",
   "id": "6d2a0d4414f5ff48",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label * 2",
   "id": "70f2f1ecf594531f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.425488Z",
     "start_time": "2024-10-27T08:00:18.350963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset.targets = [[i, i] for i in train_dataset.targets]\n",
    "valid_dataset.targets = [[i, i] for i in valid_dataset.targets]\n",
    "train_dataset.samples = [(p, torch.tensor([i, i])) for p, i in train_dataset.samples]\n",
    "valid_dataset.samples = [(p, torch.tensor([i, i])) for p, i in valid_dataset.samples]"
   ],
   "id": "426b8be55912b325",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DataLoader",
   "id": "0ccaf990"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.451656Z",
     "start_time": "2024-10-27T08:00:18.448438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 200"
   ],
   "id": "b5db3d26",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.513163Z",
     "start_time": "2024-10-27T08:00:18.504686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)"
   ],
   "id": "caf78698",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Number of CPU cores - 48\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.572893Z",
     "start_time": "2024-10-27T08:00:18.567315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)"
   ],
   "id": "d3220e37",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.609215Z",
     "start_time": "2024-10-27T08:00:18.605987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#images, targets = next(iter(train_loader))\n",
    "#grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "#imshow(grid_images)"
   ],
   "id": "566c71c2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Model",
   "id": "e26620dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.663859Z",
     "start_time": "2024-10-27T08:00:18.657530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VisualEmbedding(nn.Module):\n",
    "    \"\"\" Visual Embedding Model \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.resnet.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ],
   "id": "d3075552644b9a08",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "ca47ef1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:18.728214Z",
     "start_time": "2024-10-27T08:00:18.721770Z"
    }
   },
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, comb_axis: int, num_combinations: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Visual Embedding\n",
    "        self.visual_embedding = VisualEmbedding(embedding_dim)\n",
    "        self.semantic_embedding = nn.Linear(embedding_dim, comb_axis)\n",
    "        self.converter = nn.Linear(comb_axis, num_combinations)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        embedding = self.visual_embedding(x)\n",
    "        embedding = self.semantic_embedding(embedding)\n",
    "        return self.converter(embedding)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:19.051938Z",
     "start_time": "2024-10-27T08:00:19.047961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class ImageClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim: int, comb_axis: int, num_combinations: int, num_classes: int):\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "# \n",
    "#         # Visual Embedding\n",
    "#         self.visual_embedding = VisualEmbedding(embedding_dim)\n",
    "#         self.hidden_size = self.visual_embedding.resnet.fc.in_features\n",
    "#         self.semantic_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(embedding_dim),\n",
    "#             nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(self.hidden_size//2, comb_axis),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x) -> torch.Tensor:\n",
    "#         embedding = self.visual_embedding(x)\n",
    "#         return self.semantic_embedding(embedding) * self.num_classes"
   ],
   "id": "e564faf1e829e56d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:19.134926Z",
     "start_time": "2024-10-27T08:00:19.129631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBEDDING_DIM = 16  # 8~16: log(labels)\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    embedding_dim=EMBEDDING_DIM, comb_axis=COMBINATION_AXIS,\n",
    "    num_combinations=label_transformer.num_combinations, num_classes=CLASS_LABELS\n",
    ")"
   ],
   "id": "6ff5435a6ee3c341",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "81ca0fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:22.281167Z",
     "start_time": "2024-10-27T08:00:19.161847Z"
    }
   },
   "source": [
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (visual_embedding): VisualEmbedding(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (semantic_embedding): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (converter): Linear(in_features=2, out_features=7503, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "17163e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:22.332542Z",
     "start_time": "2024-10-27T08:00:22.326974Z"
    }
   },
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 10\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCH)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "e3bee1b6",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:22.380585Z",
     "start_time": "2024-10-27T08:00:22.376317Z"
    }
   },
   "cell_type": "code",
   "source": "#wandb.watch(model, criterion, log=\"all\", log_freq=10)",
   "id": "cdfc36ab08a8cdcc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:22.440855Z",
     "start_time": "2024-10-27T08:00:22.432317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    losses = []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    line, = ax.plot(losses)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(new_loss):\n",
    "        losses.append(new_loss.item())\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(len(losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "id": "f722e10a",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "d5bf15c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T08:00:40.236748Z",
     "start_time": "2024-10-27T08:00:22.494284Z"
    }
   },
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(EPOCH), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "      tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            update(loss)\n",
    "            train_progress.update(1)\n",
    "            #if i != train_length-1: wandb.log({'Loss': loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCH}], Step [{i+1:2}/{train_length}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / len(valid_dataset)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        #wandb.log({'Loss': loss.item(), 'Val Acc': val_acc, 'Val Loss': val_loss})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCH}], Step [{train_length}/{train_length}], Loss: {loss.item():.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCH else \"\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f12b09dec784762a987d27d6a4bc915"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/38 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78d522288deb4fe39831444a8240c766"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec61d582d32c4e2fa8920e428cb1311c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ec76b9d3eb0479eae5371ee82521022"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([200, 2])) that is different to the input size (torch.Size([200, 7503])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7503) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), targets\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m---> 20\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[1;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:538\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mmse_loss(\u001B[38;5;28minput\u001B[39m, target, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction)\n",
      "File \u001B[0;32m/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3383\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3381\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3383\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbroadcast_tensors(\u001B[38;5;28minput\u001B[39m, target)\n\u001B[1;32m   3384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[0;32m/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torch/functional.py:77\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[0;34m(*tensors)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mbroadcast_tensors(tensors)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (7503) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "0e02f1a4",
   "metadata": {},
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"visual_embedding.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a063dc95",
   "metadata": {},
   "source": "# Model Evaluation"
  },
  {
   "cell_type": "code",
   "id": "421f7090",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "model_id = \"visual_embedding\"\n",
    "\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e391d63",
   "metadata": {},
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        results['id'] += [test_dataset.classes[i] for i in ids]\n",
    "        results['label1'] += preds.cpu().detach().numpy().tolist()\n",
    "        results['label2'] += preds.cpu().detach().numpy().tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-arrange Results\n",
    "for i, label in enumerate(results['label1']):\n",
    "    results['label1'][i], results['label2'][i] = label_transformer.find(label)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "1f8634ab05729dcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_ids, _preds = [], []\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _ids.extend([test_dataset.classes[i] for i in ids])\n",
    "        _preds.extend(model.predict_top_k(inputs, k=2, min_similarity=0.3))"
   ],
   "id": "4e8af89acb2aadf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "for i, labels in zip(_ids, _preds):\n",
    "    results['id'].append(i)\n",
    "    labels = [-2 if v == CLASS_LABELS-1 else v for v in (labels[0].item(), labels[1].item())]\n",
    "    results['label1'].append(min(labels))\n",
    "    results['label2'].append(max(labels))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "bd38e6afae313549"
  },
  {
   "cell_type": "code",
   "id": "c96e7285",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "submission_dir = \"submissions\"\n",
    "if not path.isdir(submission_dir):\n",
    "    mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = path.join(submission_dir, f\"{model_id}.csv\")\n",
    "results_df.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85668afc",
   "metadata": {},
   "source": [
    "def calculate_score(submitted: pd.DataFrame, answer: pd.DataFrame) -> float:\n",
    "    total_cases = len(answer)\n",
    "    correct_cases = 0\n",
    "    for _id in answer['id']:\n",
    "        try:\n",
    "            if submitted[submitted['id'] == _id].reset_index(drop=True).equals(answer[answer['id'] == _id].reset_index(drop=True)):\n",
    "                correct_cases += 1\n",
    "        except KeyError as ignored:\n",
    "            pass  # The case when the id is not found in the submitted data\n",
    "    return correct_cases / total_cases * 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "calculate_score(pd.read_csv(\"submissions/avisual_embedding.csv\"), pd.read_csv(\"submissions/cc.csv\"))",
   "id": "b3bf32ee1c39a04e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
