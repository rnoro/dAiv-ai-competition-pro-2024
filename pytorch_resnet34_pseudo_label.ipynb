{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f4de42",
   "metadata": {},
   "source": [
    "# dAiv AI_Competition[2024]_Pro Baseline for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9142930",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd45d5b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:07:54.282155Z",
     "start_time": "2024-10-16T00:07:50.443324Z"
    }
   },
   "source": [
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils, models\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets.utils.tqdm = tqdm\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4049b434",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "e475e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:07:54.922615Z",
     "start_time": "2024-10-16T00:07:54.298453Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 16 00:07:54 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    34W / 250W |    679MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    31W / 250W |  15369MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   61C    P0    89W / 250W |  12913MiB / 16280MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    32W / 250W |  15121MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1   N/A  N/A      9377      C   ...on08/anaconda3/bin/python    15367MiB |\r\n",
      "|    3   N/A  N/A     30807      C   ...on03/anaconda3/bin/python    12911MiB |\r\n",
      "|    4   N/A  N/A     26838      C   ...on04/anaconda3/bin/python    15119MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "0d2df737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:08:12.210563Z",
     "start_time": "2024-10-16T00:07:55.569209Z"
    }
   },
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 7\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    random_state = 20241028\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train or valid:  # Split Train and Validation Set\n",
    "            seperated = train_test_split(\n",
    "                self.samples, self.targets, test_size=1-split_ratio, stratify=self.targets, random_state=self.random_state\n",
    "            )\n",
    "            self.samples, self.targets = (seperated[0], seperated[2]) if train else (seperated[1], seperated[3])\n",
    "            self.imgs = self.samples\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "\n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")\n",
    "\n",
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])\n",
    "\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")\n",
    "\n",
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1\n",
    "\n",
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    resizer\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")\n",
    "\n",
    "# Set Batch Size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "\n",
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "# imshow(grid_images)\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    losses = []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    line, = ax.plot(losses)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(new_loss):\n",
    "        losses.append(new_loss.item())\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(len(losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None\n",
    "\n",
    "class SecondMaxLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_copy = x.clone()\n",
    "        max_val, max_idx = torch.max(x_copy, dim=1, keepdim=True)\n",
    "        x_copy = x_copy.scatter(1, max_idx, -float('inf'))\n",
    "        return x_copy\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, input_channel: int, output_channel: int, adaptive_pool_size: int, img_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.multiple_output = False\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.fc_size = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Fully connected layer 제거\n",
    "\n",
    "        # Adaptive Layer\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(self.fc_size, adaptive_pool_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Label Predictor\n",
    "        self.classifier = nn.Linear(adaptive_pool_size, num_classes)\n",
    "        self.secondary = SecondMaxLayer()  # For multi-label classification\n",
    "\n",
    "        # Domain Classifier\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(adaptive_pool_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def toggle_multilabel(self, multi_label: bool = None):\n",
    "        if isinstance(multi_label, bool):\n",
    "            self.multiple_output = multi_label\n",
    "        else:\n",
    "            self.multiple_output = not self.multiple_output\n",
    "\n",
    "    def forward(self, x, alpha=0.0):\n",
    "        # Feature Extraction\n",
    "        features = self.resnet(x)\n",
    "        features = self.feature_extractor(features)\n",
    "    \n",
    "        # Label Prediction\n",
    "        class_output = self.classifier(features)\n",
    "        if self.multiple_output:\n",
    "            secondary_output = self.secondary(class_output)\n",
    "        else:\n",
    "            secondary_output = None\n",
    "    \n",
    "        # Domain Classification with Gradient Reversal\n",
    "        reverse_feature = GradientReversalFunction.apply(features, alpha)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "    \n",
    "        return domain_output, class_output, secondary_output\n",
    "\n",
    "\n",
    "\n",
    "CLASS_LABELS = len(train_dataset.classes)\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    input_channel=3, output_channel=64, adaptive_pool_size=512,\n",
    "    img_size=IMG_SIZE[0], num_classes=CLASS_LABELS\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.toggle_multilabel(True)\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset loaded successfully. Number of samples - Train(7478), Valid(1870), Test(1110), Unlabeled(380)\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Train dataset has been overridden with augmented state. Number of samples - Train(7478)\n",
      "INFO: Number of CPU cores - 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-16T00:08:12.284259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MultiLabelWrapper 클래스 정의 (오타 수정: Dataset 상속)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiLabelWrapper(Dataset):\n",
    "    def __init__(self, dataset, num_classes):\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        multilabel = torch.zeros(self.num_classes)\n",
    "        multilabel[label] = 1\n",
    "        return image, multilabel\n",
    "\n",
    "# 모든 데이터셋을 멀티 레이블 형식으로 변환\n",
    "train_dataset_ml = MultiLabelWrapper(train_dataset, CLASS_LABELS)\n",
    "valid_dataset_ml = MultiLabelWrapper(valid_dataset, CLASS_LABELS)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset_ml, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset_ml, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set Epoch Count\n",
    "num_epochs = 30\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "train_length = len(train_loader)\n",
    "valid_length = len(valid_loader)\n",
    "\n",
    "epochs = tqdm(range(num_epochs), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "      tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    # update = create_plot()  # Create Loss Plot (필요한 경우 활성화)\n",
    "\n",
    "    # Set up for training\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # 단일 레이블 데이터로 학습\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            domain_output, class_output, _ = model(inputs)\n",
    "            loss = criterion(class_output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_progress.update(1)\n",
    "\n",
    "            print(f\"\\rEpoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\", end=\"\")\n",
    "\n",
    "        # 비지도 데이터에 대한 가상 레이블 생성\n",
    "        model.eval()\n",
    "        pseudo_labels_list = []\n",
    "        inputs_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in unlabeled_loader:\n",
    "                inputs, _ = data  # 레이블은 사용하지 않으므로 '_'로 무시\n",
    "                inputs = inputs.to(device)\n",
    "                domain_output, class_output, _ = model(inputs)\n",
    "                probs = torch.sigmoid(class_output)\n",
    "\n",
    "                # 임계값을 사용하여 가상 레이블 생성 (예: 0.7 이상인 클래스만 선택)\n",
    "                threshold = 0.7\n",
    "                pseudo_labels = (probs >= threshold).float()\n",
    "\n",
    "                # 가상 레이블이 없는 샘플은 제외\n",
    "                if pseudo_labels.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                pseudo_labels_list.append(pseudo_labels.cpu())\n",
    "                inputs_list.append(inputs.cpu())\n",
    "\n",
    "            # 가상 레이블 데이터셋 생성\n",
    "            if pseudo_labels_list:\n",
    "                pseudo_dataset = torch.utils.data.TensorDataset(\n",
    "                    torch.cat(inputs_list), torch.cat(pseudo_labels_list)\n",
    "                )\n",
    "            else:\n",
    "                pseudo_dataset = None\n",
    "\n",
    "        if pseudo_dataset is not None:\n",
    "            # train_dataset_ml과 pseudo_dataset을 결합\n",
    "            combined_dataset = ConcatDataset([train_dataset_ml, pseudo_dataset])\n",
    "            combined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "        else:\n",
    "            combined_loader = train_loader\n",
    "\n",
    "        # 모델 재학습\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        train_progress.reset(total=len(combined_loader))\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(combined_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            domain_output, class_output, _ = model(inputs)\n",
    "\n",
    "            # 손실 함수 적용 (모든 데이터에 대해 동일하게)\n",
    "            loss = criterion(class_output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # update(loss)  # 필요한 경우 활성화\n",
    "            train_progress.update(1)\n",
    "\n",
    "            # 스텝별 손실 출력\n",
    "            print(f\"\\rEpoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(combined_loader)}], Loss: {loss.item():.4f}\", end=\"\")\n",
    "\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        # 검증 루프\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                domain_output, class_output, _ = model(inputs)\n",
    "                loss = criterion(class_output, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "                # 예측 및 레이블 저장\n",
    "                preds = torch.sigmoid(class_output)\n",
    "                preds = (preds >= 0.5).float()\n",
    "                all_labels.append(labels.cpu())\n",
    "                all_preds.append(preds.cpu())\n",
    "\n",
    "                valid_progress.update(1)\n",
    "        \n",
    "            val_loss /= len(valid_loader.dataset)\n",
    "        \n",
    "            # 전체 예측 및 레이블 텐서로 변환\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_preds = torch.cat(all_preds)\n",
    "        \n",
    "            # F1-스코어 계산 (매크로 평균)\n",
    "            val_f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro') * 100\n",
    "        \n",
    "            print(f\"\\nEpoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.6f}, Validation F1 Score: {val_f1:.6f}\")"
   ],
   "id": "472cc0dec1199f3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "963da6a71b2045d4bed5a52d09f5d842"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/59 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe468dad99da41c097f9d1d72c147b6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63c7fa95983b43c7a47a29bab1ae64e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [59/59], Loss: 0.0454\n",
      "Epoch [1/30], Validation Loss: 0.038312, Validation F1 Score: 0.000000\n",
      "Epoch [2/30], Step [60/60], Loss: 0.0285\n",
      "Epoch [2/30], Validation Loss: 0.025029, Validation F1 Score: 12.217657\n",
      "Epoch [3/30], Step [62/62], Loss: 0.0237\n",
      "Epoch [3/30], Validation Loss: 0.020067, Validation F1 Score: 22.590933\n",
      "Epoch [4/30], Step [62/62], Loss: 0.0164\n",
      "Epoch [4/30], Validation Loss: 0.016616, Validation F1 Score: 24.412598\n",
      "Epoch [5/30], Step [62/62], Loss: 0.0122\n",
      "Epoch [5/30], Validation Loss: 0.014958, Validation F1 Score: 25.391870\n",
      "Epoch [6/30], Step [62/62], Loss: 0.0119\n",
      "Epoch [6/30], Validation Loss: 0.012258, Validation F1 Score: 27.382527\n",
      "Epoch [7/30], Step [62/62], Loss: 0.0091\n",
      "Epoch [7/30], Validation Loss: 0.010046, Validation F1 Score: 32.994706\n",
      "Epoch [8/30], Step [62/62], Loss: 0.0079\n",
      "Epoch [8/30], Validation Loss: 0.009513, Validation F1 Score: 43.836942\n",
      "Epoch [9/30], Step [62/62], Loss: 0.0063\n",
      "Epoch [9/30], Validation Loss: 0.008082, Validation F1 Score: 55.780289\n",
      "Epoch [10/30], Step [62/62], Loss: 0.0034\n",
      "Epoch [10/30], Validation Loss: 0.007535, Validation F1 Score: 64.679749\n",
      "Epoch [11/30], Step [62/62], Loss: 0.0045\n",
      "Epoch [11/30], Validation Loss: 0.008413, Validation F1 Score: 67.909710\n",
      "Epoch [12/30], Step [62/62], Loss: 0.0023\n",
      "Epoch [12/30], Validation Loss: 0.007879, Validation F1 Score: 69.696672\n",
      "Epoch [13/30], Step [62/62], Loss: 0.0037\n",
      "Epoch [13/30], Validation Loss: 0.008117, Validation F1 Score: 70.987491\n",
      "Epoch [14/30], Step [62/62], Loss: 0.0009\n",
      "Epoch [14/30], Validation Loss: 0.007591, Validation F1 Score: 73.857238\n",
      "Epoch [15/30], Step [62/62], Loss: 0.0018\n",
      "Epoch [15/30], Validation Loss: 0.008338, Validation F1 Score: 70.160536\n",
      "Epoch [16/30], Step [62/62], Loss: 0.0023\n",
      "Epoch [16/30], Validation Loss: 0.008890, Validation F1 Score: 69.878892\n",
      "Epoch [17/30], Step [62/62], Loss: 0.0011\n",
      "Epoch [17/30], Validation Loss: 0.008157, Validation F1 Score: 72.676316\n",
      "Epoch [18/30], Step [62/62], Loss: 0.0003\n",
      "Epoch [18/30], Validation Loss: 0.008443, Validation F1 Score: 73.998565\n",
      "Epoch [19/30], Step [10/59], Loss: 0.0006"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 62\u001B[0m\n\u001B[1;32m     59\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     60\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 62\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     63\u001B[0m train_progress\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Step [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:34:21.899198Z",
     "start_time": "2024-10-16T00:34:21.636965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"baseline.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "0e02f1a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models/baseline.pt\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Evaluate",
   "id": "22c14950a30ff29"
  },
  {
   "cell_type": "code",
   "id": "421f7090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:34:32.736794Z",
     "start_time": "2024-10-16T00:34:31.962983Z"
    }
   },
   "source": [
    "# Load Model\n",
    "model_id = \"baseline\"\n",
    "\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_19578/946923917.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=120, bias=True)\n",
       "  (secondary): SecondMaxLayer()\n",
       "  (domain_classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "2e391d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:34:42.164072Z",
     "start_time": "2024-10-16T00:34:34.870674Z"
    }
   },
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "model.toggle_multilabel(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _, outputs1, outputs2 = model(inputs)\n",
    "\n",
    "        # outputs2에서 최대값의 인덱스를 얻습니다.\n",
    "        preds1 = torch.argmax(outputs1, dim=1)\n",
    "        preds2 = torch.argmax(outputs2, dim=1)\n",
    "\n",
    "        results['id'] += [test_dataset.classes[i] for i in ids]\n",
    "        results['label1'] += preds1.cpu().numpy().tolist()\n",
    "        results['label2'] += preds2.cpu().numpy().tolist()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe057e0f3c4b4c228c762ae9d6589363"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "model.toggle_multilabel(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _, class_output, _ = model(inputs)\n",
    "\n",
    "        # 가장 큰 두 개의 값의 인덱스를 얻습니다.\n",
    "        topk_vals, topk_indices = torch.topk(class_output, k=2, dim=1)\n",
    "        print(topk_vals, topk_indices)\n",
    "\n",
    "        # 첫 번째와 두 번째로 큰 값의 인덱스\n",
    "        preds1 = topk_indices[:, 0]\n",
    "        preds2 = topk_indices[:, 1]\n",
    "\n",
    "        results['id'] += [test_dataset.classes[i] for i in ids]\n",
    "        results['label1'] += preds1.cpu().numpy().tolist()\n",
    "        results['label2'] += preds2.cpu().numpy().tolist()"
   ],
   "id": "79df603b0b7e9a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:34:42.199933Z",
     "start_time": "2024-10-16T00:34:42.176398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Re-arrange Results and handle cases where label1 == label2\n",
    "for i, labels in enumerate(zip(results['label1'], results['label2'])):\n",
    "    label1, label2 = min(labels), max(labels)\n",
    "    if label1 == label2:\n",
    "        # label1과 label2가 같은 경우 label1을 -1로 변경\n",
    "        results['label1'][i] = -1\n",
    "        results['label2'][i] = label2\n",
    "    else:\n",
    "        results['label1'][i] = label1\n",
    "        results['label2'][i] = label2\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "1f8634ab05729dcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              id  label1  label2\n",
       "0     TEST_00000     116     118\n",
       "1     TEST_00001      58      62\n",
       "2     TEST_00002      84      94\n",
       "3     TEST_00003      16      20\n",
       "4     TEST_00004      59      81\n",
       "...          ...     ...     ...\n",
       "1105  TEST_01105       2      18\n",
       "1106  TEST_01106      66      67\n",
       "1107  TEST_01107      20      24\n",
       "1108  TEST_01108      28      29\n",
       "1109  TEST_01109      15      95\n",
       "\n",
       "[1110 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>116</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>84</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>59</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>TEST_01105</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>TEST_01106</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>TEST_01107</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>TEST_01108</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>TEST_01109</td>\n",
       "      <td>15</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "c96e7285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:34:42.309234Z",
     "start_time": "2024-10-16T00:34:42.297657Z"
    }
   },
   "source": [
    "# Save Results\n",
    "submission_dir = \"submissions\"\n",
    "if not path.isdir(submission_dir):\n",
    "    mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = path.join(submission_dir, f\"{model_id}.csv\")\n",
    "results_df.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to submissions/baseline.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "85668afc",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
