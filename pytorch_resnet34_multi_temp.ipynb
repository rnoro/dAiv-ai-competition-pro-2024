{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f4de42",
   "metadata": {},
   "source": [
    "# dAiv AI_Competition[2024]_Pro Baseline for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9142930",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd45d5b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T14:56:45.437028Z",
     "start_time": "2024-10-15T14:56:41.588691Z"
    }
   },
   "source": [
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils, models\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# datasets.utils.tqdm = tqdm\n",
    "# %matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4049b434",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "e475e84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T14:57:10.191456Z",
     "start_time": "2024-10-15T14:57:09.540498Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 15 14:57:09 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    34W / 250W |   7724MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:06:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    30W / 250W |  15369MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  On   | 00000000:07:00.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  On   | 00000000:08:00.0 Off |                    0 |\r\n",
      "| N/A   59C    P0    44W / 250W |  12913MiB / 16280MiB |     86%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   4  Tesla P100-PCIE...  On   | 00000000:0C:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    32W / 250W |  15089MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   5  Tesla P100-PCIE...  On   | 00000000:0D:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   6  Tesla P100-PCIE...  On   | 00000000:0E:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   7  Tesla P100-PCIE...  On   | 00000000:0F:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     12525      C   ...on01/anaconda3/bin/python     7045MiB |\r\n",
      "|    1   N/A  N/A      9377      C   ...on08/anaconda3/bin/python    15367MiB |\r\n",
      "|    3   N/A  N/A     43272      C   ...on03/anaconda3/bin/python    12911MiB |\r\n",
      "|    4   N/A  N/A     26838      C   ...on04/anaconda3/bin/python    15087MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0d2df737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T14:57:35.590518Z",
     "start_time": "2024-10-15T14:57:15.909874Z"
    }
   },
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 7\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    random_state = 20241028\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train or valid:  # Split Train and Validation Set\n",
    "            seperated = train_test_split(\n",
    "                self.samples, self.targets, test_size=1-split_ratio, stratify=self.targets, random_state=self.random_state\n",
    "            )\n",
    "            self.samples, self.targets = (seperated[0], seperated[2]) if train else (seperated[1], seperated[3])\n",
    "            self.imgs = self.samples\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "\n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")\n",
    "\n",
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])\n",
    "\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")\n",
    "\n",
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1\n",
    "\n",
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    resizer\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")\n",
    "\n",
    "# Set Batch Size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "\n",
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "# imshow(grid_images)\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    losses = []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    line, = ax.plot(losses)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(new_loss):\n",
    "        losses.append(new_loss.item())\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(len(losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None\n",
    "\n",
    "class SecondMaxLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clone()\n",
    "        max_val, max_idx = torch.max(x, dim=1, keepdim=True)\n",
    "        x.scatter_(1, max_idx, 1e-12)\n",
    "        return x\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, input_channel: int, output_channel: int, adaptive_pool_size: int, img_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.multiple_output = False\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.resnet = models.resnet34(pretrained=True)\n",
    "        self.fc_size = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()  # Fully connected layer 제거\n",
    "\n",
    "        # Adaptive Layer\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(self.fc_size, adaptive_pool_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Label Predictor\n",
    "        self.classifier = nn.Linear(adaptive_pool_size, num_classes)\n",
    "        self.secondary = SecondMaxLayer()  # For multi-label classification\n",
    "\n",
    "        # Domain Classifier\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(adaptive_pool_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def toggle_multilabel(self, multi_label: bool = None):\n",
    "        if isinstance(multi_label, bool):\n",
    "            self.multiple_output = multi_label\n",
    "        else:\n",
    "            self.multiple_output = not self.multiple_output\n",
    "\n",
    "    def forward(self, x, alpha=0.0):\n",
    "        # Feature Extraction\n",
    "        features = self.resnet(x)\n",
    "        features = self.feature_extractor(features)\n",
    "\n",
    "        # Label Prediction\n",
    "        class_output = self.classifier(features)\n",
    "        if self.multiple_output:\n",
    "            secondary_output = self.secondary(class_output)\n",
    "        else:\n",
    "            secondary_output = None\n",
    "\n",
    "        # Domain Classification with Gradient Reversal\n",
    "        reverse_feature = GradientReversalFunction.apply(features, alpha)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return domain_output, class_output, secondary_output\n",
    "\n",
    "\n",
    "CLASS_LABELS = len(train_dataset.classes)\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    input_channel=3, output_channel=64, adaptive_pool_size=512,\n",
    "    img_size=IMG_SIZE[0], num_classes=CLASS_LABELS\n",
    ")\n",
    "\n",
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.toggle_multilabel(True)\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "label_criterion = nn.BCEWithLogitsLoss()  # 멀티 레이블 분류를 위한 손실 함수\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "# Set Epoch Count\n",
    "num_epochs = 10"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Dataset loaded successfully. Number of samples - Train(7478), Valid(1870), Test(1110), Unlabeled(380)\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Train dataset has been overridden with augmented state. Number of samples - Train(7478)\n",
      "INFO: Number of CPU cores - 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/shared_hdd/rnoro5122/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T15:01:33.087391Z",
     "start_time": "2024-10-15T15:01:21.346476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(num_epochs), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "      tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    # update = create_plot()  # Create Loss Plot (이 함수가 정의되어 있어야 합니다.)\n",
    "\n",
    "    from itertools import zip_longest\n",
    "\n",
    "    # Set up for training\n",
    "    alpha = 0.0  # Gradient Reversal 강도 조절\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        model.train()\n",
    "        len_dataloader = min(len(train_loader), len(unlabeled_loader))\n",
    "        train_iter = iter(train_loader)\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "        for i in range(len_dataloader):\n",
    "            # 데이터 로드\n",
    "            data_source = next(train_iter)\n",
    "            data_target = next(unlabeled_iter)\n",
    "\n",
    "            # 레이블이 있는 데이터 (소스 도메인)\n",
    "            inputs_source, labels_source = data_source\n",
    "            inputs_source, labels_source = inputs_source.to(device), labels_source.to(device)\n",
    "            domain_labels_source = torch.zeros(inputs_source.size(0), 1).to(device)  # 소스 도메인 레이블: 0\n",
    "\n",
    "            # 레이블이 없는 데이터 (타겟 도메인)\n",
    "            inputs_target = data_target\n",
    "\n",
    "            # inputs_target이 리스트인 경우 텐서로 변환\n",
    "            if isinstance(inputs_target, list):\n",
    "                inputs_target = torch.stack(inputs_target, dim=0)\n",
    "            elif isinstance(inputs_target, tuple):\n",
    "                inputs_target = inputs_target[0]\n",
    "\n",
    "            inputs_target = inputs_target.to(device)\n",
    "            domain_labels_target = torch.ones(inputs_target.size(0), 1).to(device)  # 타겟 도메인 레이블: 1\n",
    "\n",
    "            # 모델 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 소스 도메인 데이터로 예측\n",
    "            p = float(i + epoch * len_dataloader) / (num_epochs * len_dataloader)\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1  # GRL의 스케줄링 파라미터\n",
    "            domain_output_s, class_output_s, _ = model(inputs_source, alpha=alpha)\n",
    "            # 타겟 도메인 데이터로 도메인 분류기 예측\n",
    "            domain_output_t, _, _ = model(inputs_target, alpha=alpha)\n",
    "\n",
    "            # 손실 계산\n",
    "            # 레이블 예측 손실 (소스 도메인)\n",
    "            label_loss = label_criterion(class_output_s, labels_source.float())\n",
    "            # 도메인 분류 손실 (소스 + 타겟 도메인)\n",
    "            domain_output = torch.cat((domain_output_s, domain_output_t), 0)\n",
    "            domain_labels = torch.cat((domain_labels_source, domain_labels_target), 0)\n",
    "            domain_loss = domain_criterion(domain_output, domain_labels)\n",
    "\n",
    "            # 총 손실\n",
    "            loss = label_loss + domain_loss\n",
    "\n",
    "            # update(loss)  # 이 부분은 주석 처리 또는 함수 정의 필요\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len_dataloader}], Loss: {loss.item():.4f}\", end=\"\")\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 검증 루프\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # 예측\n",
    "                _, outputs, _ = model(inputs)\n",
    "\n",
    "                # 손실 계산\n",
    "                loss = label_criterion(outputs, labels.float())\n",
    "                val_loss += loss.item() * batch_size  # 배치 손실의 합\n",
    "\n",
    "                # 정확도 계산\n",
    "                y_pred = torch.sigmoid(outputs)\n",
    "                y_pred = (y_pred > 0.5).float()\n",
    "                correct = (y_pred == labels).float().sum()  # 배치에서 맞은 예측의 수\n",
    "                val_acc += correct.item()\n",
    "\n",
    "        # 평균 손실 및 정확도 계산\n",
    "        val_loss = val_loss / total_samples\n",
    "        val_acc = val_acc / (total_samples * labels.size(1))  # 멀티 레이블이므로 클래스 수로 나눔\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch+1:2}/{num_epochs}], Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_acc:.6%}\")"
   ],
   "id": "472cc0dec1199f3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Running Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a83166a327154745be178392f2bf5d36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/59 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24563037eeb6410d81b57ba74c579a9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d18934962a4f4858b32a6bfa7670d630"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [128, 3, 256, 256] at entry 0 and [128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 36\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# inputs_target이 리스트인 경우 텐서로 변환\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs_target, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m---> 36\u001B[0m     inputs_target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(inputs_target, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs_target, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m     38\u001B[0m     inputs_target \u001B[38;5;241m=\u001B[39m inputs_target[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [128, 3, 256, 256] at entry 0 and [128] at entry 1"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"test.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "id": "0e02f1a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a063dc95",
   "metadata": {},
   "source": "# Model Evaluation"
  },
  {
   "cell_type": "code",
   "id": "421f7090",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "model_id = \"test\"\n",
    "\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e391d63",
   "metadata": {},
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "model.toggle_multilabel(True)\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _, outputs1, outputs2 = model(inputs)\n",
    "        preds1, preds2 = torch.max(outputs1, 1)[1], torch.max(outputs2, 1)[1]\n",
    "        results['id'] += [test_dataset.classes[i] for i in ids]\n",
    "        results['label1'] += preds1.cpu().detach().numpy().tolist()\n",
    "        results['label2'] += preds2.cpu().detach().numpy().tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model_with_single_and_multi_labels(model, dataloader, device, test_dataset):\n",
    "    \"\"\"\n",
    "    모델의 추론 결과를 id, label1, label2로 저장합니다.\n",
    "    - 단일 레이블일 경우 label1에는 -1, label2에는 해당 레이블이 들어가야 합니다.\n",
    "    - 멀티 레이블일 경우 두 레이블이 오름차순으로 정렬되어 label1과 label2에 들어갑니다.\n",
    "    \"\"\"\n",
    "    results = dict(id=[], label1=[], label2=[])\n",
    "\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    model.toggle_multilabel(True)  # 멀티 레이블 분류 활성화\n",
    "\n",
    "    with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "        for inputs, ids in tqdm(dataloader):  # test_loader에서 입력과 id를 가져옴\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # 모델 출력\n",
    "            _, outputs1, outputs2 = model(inputs)\n",
    "\n",
    "            # 각 출력의 예측 클래스 계산\n",
    "            preds1 = torch.max(outputs1, 1)[1]  # outputs1에서 가장 높은 값을 가지는 클래스\n",
    "            preds2 = torch.max(outputs2, 1)[1]  # outputs2에서 가장 높은 값을 가지는 클래스\n",
    "\n",
    "            # 각 배치의 예측 결과 처리\n",
    "            for i in range(len(preds1)):\n",
    "                id_ = test_dataset.classes[ids[i]]  # 해당 샘플의 id\n",
    "\n",
    "                if preds1[i] == preds2[i]:  # 단일 레이블인 경우\n",
    "                    label1, label2 = -1, preds1[i].item()\n",
    "                else:  # 멀티 레이블인 경우\n",
    "                    label1, label2 = sorted([preds1[i].item(), preds2[i].item()])\n",
    "\n",
    "                # 결과 저장\n",
    "                results['id'].append(id_)\n",
    "                results['label1'].append(label1)\n",
    "                results['label2'].append(label2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 모델을 평가하는 함수 호출 예시\n",
    "results = evaluate_model_with_single_and_multi_labels(model, test_loader, device, test_dataset)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ],
   "id": "fc43f792a924b6aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Re-arrange Results\n",
    "for i, labels in enumerate(zip(results['label1'], results['label2'])):\n",
    "    #results['label1'][i], results['label2'][i] = min(labels), max(labels)\n",
    "    results['label1'][i], results['label2'][i] = -1, results['label1'][i]  # 멀티 라벨 분류 안하고 그냥 '-1, 라벨'로 처리\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "id": "1f8634ab05729dcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c96e7285",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "submission_dir = \"submissions\"\n",
    "if not path.isdir(submission_dir):\n",
    "    mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = path.join(submission_dir, f\"test.csv\")\n",
    "results_df.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85668afc",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
