{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f4de42",
   "metadata": {},
   "source": [
    "# dAiv AI_Competition[2024]_Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9142930",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd374cd8ed6f6ddc",
   "metadata": {},
   "source": [
    "#%pip install pygwalker wandb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd45d5b6",
   "metadata": {},
   "source": [
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "datasets.utils.tqdm = tqdm\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3934456c3880d88",
   "metadata": {},
   "source": [
    "# WandB Initialization\n",
    "#wandb.init(project=\"dAiv-ai-competition-2024-pro\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4049b434",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "id": "e475e84e",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d2df737",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85bc3d0f",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "id": "56b28aef",
   "metadata": {},
   "source": [
    "from typing import Callable, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    random_state = 20241028\n",
    "\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train or valid:  # Split Train and Validation Set\n",
    "            seperated = train_test_split(\n",
    "                self.samples, self.targets, test_size=1-split_ratio, stratify=self.targets, random_state=self.random_state\n",
    "            )\n",
    "            self.samples, self.targets = (seperated[0], seperated[2]) if train else (seperated[1], seperated[3])\n",
    "            self.imgs = self.samples\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "\n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b419ce2cb5914ac5",
   "metadata": {},
   "source": [
    "### Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1489d0b",
   "metadata": {},
   "source": [
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55933cbc",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b6fe305cb5b234b",
   "metadata": {},
   "source": [
    "### Visualize Dataset Distribution\n",
    "    - for checking..."
   ]
  },
  {
   "cell_type": "code",
   "id": "2a012af140f88d5d",
   "metadata": {},
   "source": [
    "# Label Check\n",
    "for i, label in zip(range(5), train_dataset.targets):\n",
    "    print(i, \"-\", train_dataset.classes[label])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba2d5dcc2de29df3",
   "metadata": {},
   "source": [
    "train_dataset.df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8c7a2e166c6556a",
   "metadata": {},
   "source": [
    "# Train Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c16ee12b11a6719",
   "metadata": {},
   "source": [
    "valid_dataset.df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab82cd5bc721d19c",
   "metadata": {},
   "source": [
    "# Valid Dataset Distribution\n",
    "walker = pyg.walk(valid_dataset.df, theme_key=\"streamlit\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b34674f",
   "metadata": {},
   "source": [
    "## Data Augmentation if needed"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ced1da0",
   "metadata": {},
   "source": [
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "268d5c43",
   "metadata": {},
   "source": [
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    resizer\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efd6c21a",
   "metadata": {},
   "source": [
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ccaf990",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5db3d26",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 128"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caf78698",
   "metadata": {},
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3220e37",
   "metadata": {},
   "source": [
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "566c71c2",
   "metadata": {},
   "source": [
    "#images, targets = next(iter(train_loader))\n",
    "#grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "#imshow(grid_images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e26620dd",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3075552644b9a08",
   "metadata": {},
   "source": [
    "class VisualEmbedding(nn.Module):\n",
    "    \"\"\" Visual Embedding Model \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Image Embedding\n",
    "        self.image_embedding = models.resnet34(pretrained=True)\n",
    "        self.image_embedding.avgpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.hidden_size = self.image_embedding.fc.in_features\n",
    "        self.image_embedding.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.hidden_size//2, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "        # Semantic Class Lookup Table\n",
    "        self.class_lookup = nn.Parameter(\n",
    "            torch.randn(num_classes, embedding_dim)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.class_lookup)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.image_embedding(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca47ef1a",
   "metadata": {},
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_classes: int, use_softmax=False):\n",
    "        super().__init__()\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "        # Visual Embedding\n",
    "        self.visual_embedding = VisualEmbedding(num_classes, embedding_dim)\n",
    "        self.class_lookup = self.visual_embedding.class_lookup\n",
    "\n",
    "        # Cosine Similarity Temperature\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 0.2)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        img_embeddings = F.normalize(self.visual_embedding(x), p=2, dim=1)\n",
    "        cls_embeddings = F.normalize(self.class_lookup, p=2, dim=1)\n",
    "        return torch.mm(img_embeddings, cls_embeddings.t()) / self.temperature  # find embedding location\n",
    "\n",
    "    def predict_top_k(self, x, k=2, threshold=0.5, min_similarity=0.3):\n",
    "        if self.use_softmax:\n",
    "            return self.predict_top_k_by_threshold(x, k=k, threshold=threshold)\n",
    "        else:\n",
    "            return self.predict_top_k_by_similarity(x, k=k, min_similarity=min_similarity)\n",
    "\n",
    "    def predict_top_k_by_threshold(self, x, k=2, threshold=0.5):\n",
    "        similarity = self(x)\n",
    "        probabilities = F.softmax(similarity, dim=1)\n",
    "\n",
    "        top_probs, top_classes = torch.topk(probabilities, k, dim=1)\n",
    "\n",
    "        if k > 1:  # one class prediction\n",
    "            relative_probs = top_probs[:, 1] / top_probs[:, 0]\n",
    "            mask = relative_probs < threshold\n",
    "            top_classes[mask, 1] = -1\n",
    "\n",
    "        return top_classes\n",
    "\n",
    "    def predict_top_k_by_similarity(self, x, k=2, min_similarity=0.3):\n",
    "        similarity = self(x)  # cos range (-1 ~ 1)\n",
    "        detected_classes, detected_scores = [], []\n",
    "\n",
    "        for _ in range(similarity.size(0)):\n",
    "            scores, classes = similarity[i].sort(descending=True)\n",
    "\n",
    "            mask = scores >= min_similarity\n",
    "            valid_classes = classes[mask][:k]  # clip k by similarity\n",
    "            valid_scores = scores[mask][:k]\n",
    "\n",
    "            if len(valid_classes) < k:\n",
    "                padding = torch.full((k-len(valid_classes),), -1, device=device)\n",
    "                valid_classes = torch.cat([valid_classes, padding])\n",
    "                valid_scores = torch.cat([valid_scores, torch.zeros_like(padding)])\n",
    "\n",
    "            detected_classes.append(valid_classes)\n",
    "            detected_scores.append(valid_scores)\n",
    "\n",
    "        return torch.stack(detected_classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ff5435a6ee3c341",
   "metadata": {},
   "source": [
    "CLASS_LABELS = len(train_dataset.classes) + 1\n",
    "EMBEDDING_DIM = 16\n",
    "USE_SOFTMAX = False\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    embedding_dim=EMBEDDING_DIM, num_classes=CLASS_LABELS, use_softmax=USE_SOFTMAX\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81ca0fa4",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CrossContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.1, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.alpha = alpha\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, similarity, labels):\n",
    "        ce_loss = self.cross_entropy(similarity, labels)\n",
    "\n",
    "        batch_size = similarity.size(0)\n",
    "        pos_mask = torch.zeros_like(similarity, dtype=torch.bool)  # let be the positive pair closer\n",
    "        pos_mask[torch.arange(batch_size), labels] = True\n",
    "        neg_mask = ~pos_mask  # let be the negative pair farther\n",
    "        \n",
    "        pos_similarity = similarity[pos_mask].mean()\n",
    "        neg_similarity = similarity[neg_mask].mean()\n",
    "\n",
    "        contrastive_loss = torch.clamp(neg_similarity - pos_similarity + self.margin, min=0.0)\n",
    "\n",
    "        return self.alpha * ce_loss + (1 - self.alpha) * contrastive_loss"
   ],
   "id": "d3dc0b6add3dd10a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CrossSimilarityLoss(nn.Module):\n",
    "    def __init__(self, pos_margin=0.7, neg_margin=0.3, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.pos_margin = pos_margin\n",
    "        self.neg_margin = neg_margin\n",
    "        self.alpha = alpha\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, similarity, labels):\n",
    "        ce_loss = self.cross_entropy(similarity, labels)\n",
    "        \n",
    "        batch_size = similarity.size(0)\n",
    "        pos_mask = torch.zeros_like(similarity, dtype=torch.bool)  # let be the positive pair closer\n",
    "        pos_mask[torch.arange(batch_size), labels] = True\n",
    "        neg_mask = ~pos_mask  # let be the negative pair farther\n",
    "\n",
    "        pos_loss = torch.clamp(self.pos_margin - similarity[pos_mask], min=0.0).mean()\n",
    "        neg_loss = torch.clamp(similarity[neg_mask] - self.neg_margin, min=0.0).mean()\n",
    "\n",
    "        return self.alpha * ce_loss + (1 - self.alpha) * (pos_loss + neg_loss)"
   ],
   "id": "a2cbc39c95b56310"
  },
  {
   "cell_type": "code",
   "id": "17163e17",
   "metadata": {},
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "criterion = CrossContrastiveLoss() if USE_SOFTMAX else CrossSimilarityLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3bee1b6",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "f722e10a",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    losses = []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    line, = ax.plot(losses)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(new_loss):\n",
    "        losses.append(new_loss.item())\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(len(losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdfc36ab08a8cdcc",
   "metadata": {},
   "source": [
    "#wandb.watch(model, criterion, log=\"all\", log_freq=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cdee4bd",
   "metadata": {},
   "source": [
    "# Set Epoch Count\n",
    "num_epochs = 50"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5bf15c5",
   "metadata": {},
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(num_epochs), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "      tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            update(loss)\n",
    "            train_progress.update(1)\n",
    "            #if i != train_length-1: wandb.log({'Loss': loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{num_epochs}], Step [{i+1:2}/{train_length}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / len(valid_dataset)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        #wandb.log({'Loss': loss.item(), 'Val Acc': val_acc, 'Val Loss': val_loss})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{num_epochs}], Step [{train_length}/{train_length}], Loss: {loss.item():.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == num_epochs else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e02f1a4",
   "metadata": {},
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"visual_embedding.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a063dc95",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "421f7090",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "model_id = \"visual_embedding\"\n",
    "\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e391d63",
   "metadata": {},
   "source": [
    "_ids, _preds = [], []\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _ids.extend([test_dataset.classes[i] for i in ids])\n",
    "        _preds.extend(model.predict_top_k(inputs, k=2, min_similarity=0.3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f8634ab05729dcb",
   "metadata": {},
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "for i, labels in zip(_ids, _preds):\n",
    "    results['id'].append(i)\n",
    "    labels = [-2 if v == CLASS_LABELS-1 else v for v in (labels[0].item(), labels[1].item())]\n",
    "    results['label1'].append(min(labels))\n",
    "    results['label2'].append(max(labels))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c96e7285",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "submission_dir = \"submissions\"\n",
    "if not path.isdir(submission_dir):\n",
    "    mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = path.join(submission_dir, f\"{model_id}.csv\")\n",
    "results_df.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85668afc",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
